---
title: "Bernoulli Parameters Estimation"
output: html_document
---

```{r}
library("plotly")
library("rgl")
library("dplyr")
```

Let $Y_i \in \left\{0, 1\right\}$ be independent (and identical) Bernoulli variables, such that $Y = \sum_{i=1}^{n} Y_i\sim B(n,\pi)$. Given the data $y$ we want to estimate $\pi$.

1.  Derive the ML estimate and the method of moment estimate.

**ML Estimate.** We first determine our likelihood:

$$
L(\pi ; y) = \prod_{i=1}^{n} f(y_i ; \pi) = \pi^{n_A}(1-\pi)^{n_B} = \pi^{n_A}(1-\pi)^{n - n_A}
$$

where $n_A$ and $n_B$ are the times $y_i$ is being drawn as 1 and 0 respectively, such that $n_A + n_B = n$. If $n_A =0$, we can take $\pi = 0$ because $(1-\pi)^n$ is the highest at that value. The same could be said for $n_A= 1$, which we take $\pi = 1$ instead.

Assume $n_A \neq 0, 1$. To make this easier, we instead take the log-likehood of this function, expressing it as a function of $\pi$ and explore further:

$$
\begin{align*}
l(\pi) &= n_A\log \pi + (n-n_A) \log (1 - \pi) \\
l^\prime(\pi) &= \frac{n_A}{\pi} - \frac{n-n_A}{1-\pi} = \frac{n_A(1 - \pi) - (n - n_A) \pi}{\pi(1-\pi)} \\
&= \frac{n_A - n \pi}{\pi (1- \pi)}
\end{align*}
$$

Therefore, setting $l^\prime(\pi) = 0$ , we obtained our ML estimate $\pi_{\text{ML}} = n_A/n$ that attains maximum in $L$. Since $n_A=y$, we have $\pi_{\text{ML}} = y/n$ our estimate given data $y$.

**Method of Moment Estimate.** For Bernoulli variables, calculating our empirical moments of the data is quite easy (using the same variables as before):

$$
m_k(y) = \frac{1}{n}\sum_{i=1}^{n} y_i^k = \frac{1}{n}\sum_{y_i=1} 1 = \frac{n_A}{n}
$$

for all $k \in \mathbb{N}$. Next, we want find the exact expression of the (theoretical) moments of $Y$ for comparison. However, we would not need to find the theoretical distribution for all $k$. In fact, it would be proven sufficient to just use $\mathbb{E}_\pi[Y]=\pi$. Comparing it to $m_1(y)$,

$$
\mathbb{E}_{\pi_{\text{MM}}}[Y] = \pi_{\text{MM}} = \frac{n_A}{n} = m_1(y)
$$

which we obtained $\pi_{\text{MM}} = n_A/n$. The result is quite similar to our ML estimate, which is $y/n$ given data $y$.

2.  We now look at estimates of the form

    $$
    t(y) =\frac{y+a}{a+b+n}
    $$

    where $a$ and $b$ need to be chosen appropriately. Derive the $\text{MSE}(t,\pi)$.

Through its expectation form, we have:

$$
\begin{align*}
\text{MSE}(t,\pi) &= \mathbb{E}[(t(Y) - \pi)^2] \\
&= \text{Var}_\pi(t(Y)) + \text{Bias}(t;\pi)^2 \\
&= \frac{n\pi(1-\pi)}{(a+b+n)^2} + \left(\frac{n\pi+a}{a+b+n} - \pi\right)^2 \\
&= \frac{n\pi(1-\pi)+(n\pi + a)^2}{(a+b+n)^2} - \frac{2\pi(n\pi+a)}{a+b+n} + \pi^2 \\
&= \frac{a^2 + \pi^2((a+b)^2 - n) + \pi(n-2a(a+b))}{(a+b+n)^2}
\end{align*} 
$$

where the conditions are

$$
\begin{align*}
a+b &\neq n \\
b &> 0 \\
a &> 0
\end{align*}
$$

3.  Taking the squared risk $\mathcal{L}(t,\pi)=(t-\pi)^2$, we obtain (with differentiation) the maximum risk given $a$ and $b$. Plot the risk for different values of $a$ and $b$, including $a=0$ and $b = 0$. Given your results choose the minimax estimate.

Since the $\text{MSE}$ takes after the loss $\mathcal{L}(t,\pi) = (t-\pi)^2$, we can use the previous result and differentiate in respect to $\pi$:

$$
\frac{\text{d}}{\text{d}\pi}\text{MSE}(t,\pi) = \frac{2\pi((a+b)^2 - n) + (n - 2a(a+b))}{(a+b+n)^2}
$$

which obtains us our highest risk at $\pi_0$:

$$
\pi_{0}(a,b) = \frac{2a(a+b)-n}{2(a+b)^2 - 2n}
$$

However, to check whether this is the maximum, since the MSE is a quadratic function, we need to only look at the coefficient of $\pi^2$. The only times where we would declare the highest risk is infinity is when it's positive, which is the condition $a+b>\sqrt n$

The following code calculates $(a,b,\max \text{MSE} (a,b))$ for almost every $a,b$ that satisfies

$$
\begin{align*}
a+b &\leq \sqrt n \\
b &> 0 \\
a &> 0
\end{align*}
$$

```{r}
n = 100

mse_max <- function(a, b, n){
  pi_0 <- (2*a*(a+b) - n)/(2*(a+b)^2 - 2*n)
  if_else((pi_0 >= 0) & (pi_0 <= 1),
          (a^2 + pi_0^2*((a+b)^2 - n) + pi_0*(n - 2*a*(a+b)))/((a+b+n)^2),
          NA)
}

pi_mse <- function(a, b, n){
  (2*a*(a+b) - n)/(2*(a+b)^2 - 2*n)
}

a_grid <- seq(from = 0, to = sqrt(n), length.out = 2000)
b_grid <- seq(from = 0, to = sqrt(n), length.out = 2000)

mse_max_a_b <- outer(a_grid, b_grid, FUN = "mse_max", n = n)

for(i in 1:n){
  for(j in 1:n){
    if ((a_grid[i] + b_grid[j] > sqrt(n))){
      mse_max_a_b[i,j] = NA
    }
  }
}

persp3d(mse_max_a_b, col = "skyblue")
```

```{r}
which_min = which(mse_max_a_b == min(mse_max_a_b, na.rm = TRUE), arr.ind = TRUE) 
which_mse_min = mse_max_a_b[which_min[1], which_min[2]]
print(c(a_grid[which_min[1]], b_grid[which_min[2]]))
mse_max(a_grid[which_min[1]], b_grid[which_min[2]], n) |> sqrt()
pi_mse(a_grid[which_min[1]], b_grid[which_min[2]], n)
```

The next checks whether our estimation is accurate. It seems that the minimax parameter is quite consistent, but is still close towards our target parameter. After all, it tries to minimize the estimation's maximum risk, not finding the exact estimation that minimizes risk.

```{r}
estimator <- function(y,a,b,n){
  (y + a)/(a + b + n)
}

y = rbinom(1, 10, 0.7)

cum_mean_est <- 0
lol <- 0
for(i in 1:10000){
  y = rbinom(1, 1000, 0.9996747)
  mean_est <- estimator(y, 10, 3.6618,  1000)
  real_est <- y/1000
  cum_mean_est <- (cum_mean_est*(i-1)+mean_est)/i
  lol <- (lol*(i-1)+real_est)/i
}

cum_mean_est
lol
```

The following code demonstrates that. For each probability in `probs`, we have our estimation's difference towards the real parameter, where `diff` is our estimator and `diff` is the sample mean estimator.

```{r}

probs <- seq(from = 0, to = 1, by = 0.05)
diff <- probs - probs
diff_real <- probs - probs
for(param_pi in seq(from = 0, to = 1, by = 0.05)){
  cum_mean_est <- 0
  lol <- 0
  for(i in 1:100){
    y = rbinom(1, 100, param_pi)
    mean_est <- estimator(y, 10, 3.661831,   100)
    real_est <- y/100
    cum_mean_est <- (cum_mean_est*(i-1)+mean_est)/i
    lol <- (lol*(i-1)+real_est)/i
  }
  diff[param_pi*(1/0.05)+1] <- cum_mean_est - param_pi
  diff_real[param_pi*(1/0.05)+1] <- lol - param_pi
}

data.frame(probs = probs, diff = diff, diff_real = diff_real)
diff^2 |> mean() |> sqrt()
```
